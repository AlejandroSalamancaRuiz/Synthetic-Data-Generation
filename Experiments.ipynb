{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import  SMOTE, BorderlineSMOTE, SVMSMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from numpy import ones\n",
    "from numpy import zeros\n",
    "from numpy.random import rand\n",
    "from numpy.random import randint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('heart_2020_cleaned.csv')\n",
    "\n",
    "\n",
    "#//-------------------------------------------------------------\n",
    "\n",
    "## Here the number of samples (the first parameter of the experiments) is chosen\n",
    "\n",
    "# Select number of data points to experiment\n",
    "data = data.iloc[0:1000]\n",
    "# transform labels\n",
    "labels = np.where(data['HeartDisease'] == 'Yes', 1, 0)\n",
    "# drop columns\n",
    "features = data.drop(columns = ['HeartDisease', 'KidneyDisease', 'SkinCancer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(dataframe):\n",
    "\n",
    "    categorical_var = []\n",
    "    bool_var = []\n",
    "    numeric_var = []\n",
    "\n",
    "    for col in data.columns:\n",
    "        if data[col].dtypes == 'object':\n",
    "            categorical_var.append(col)\n",
    "        elif data[col].dtypes == 'bool':\n",
    "            bool_var.append(col)\n",
    "        else:\n",
    "            numeric_var.append(col)\n",
    "\n",
    "    # Initialize an encoder\n",
    "    encoder_cat = OneHotEncoder(sparse=False)\n",
    "\n",
    "    # Transform the categorical features\n",
    "    one_hot_cat = encoder_cat.fit_transform(data[categorical_var])\n",
    "\n",
    "    # Initialize encoder\n",
    "    encoder_bool = OneHotEncoder(sparse=False, drop='first')\n",
    "\n",
    "    # Transform the boolean features\n",
    "    one_hot_bool = encoder_bool.fit_transform(data[bool_var])\n",
    "\n",
    "    # Scale numeric features with minmax scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    numeric_data = scaler.fit_transform(data[numeric_var])\n",
    "\n",
    "    # Stack all the matrices\n",
    "    new = np.hstack((one_hot_bool, one_hot_cat, numeric_data))\n",
    "\n",
    "    # Remove nan values\n",
    "    nan_indexes = np.argwhere(np.isnan(new))\n",
    "    new = np.delete(new, nan_indexes[:,0], axis=0)\n",
    "\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data_classical_models(original_data, original_labels, target_label, other_label, percentage, model_class):\n",
    "\n",
    "    model = model_class()\n",
    "\n",
    "    features_target_class = original_data[original_labels == target_label]\n",
    "\n",
    "    number_new_samples = int(features_target_class.shape[0] * percentage)\n",
    "\n",
    "    features_other_class = np.ones((features_target_class.shape[0] + number_new_samples, features_target_class.shape[1]))\n",
    "\n",
    "    X_temp = np.vstack((features_other_class, features_target_class))\n",
    "\n",
    "    y_temp = np.hstack((np.full(features_other_class.shape[0], other_label), np.full(features_target_class.shape[0], target_label)))\n",
    "\n",
    "\n",
    "    X, y = model.fit_resample(X_temp, y_temp)\n",
    "\n",
    "\n",
    "    return X[X_temp.shape[0]:,:], y[X_temp.shape[0]:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class gan1:\n",
    "\n",
    "    def __init__(self, latent_space):\n",
    "\n",
    "        self.latent_dim = latent_space\n",
    "        # create the discriminator\n",
    "        self.d_model = self.define_discriminator()\n",
    "        # create the generator\n",
    "        self.g_model = self.define_generator()\n",
    "        # create the gan\n",
    "        self.gan_model = self.define_gan()\n",
    "\n",
    "\n",
    "    # define the standalone discriminator model\n",
    "    def define_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dense(8, activation='relu'))\n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        # compile model\n",
    "        opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "\n",
    "    # define the standalone generator model\n",
    "    def define_generator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(self.latent_dim, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(20, activation='relu'))\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(52, activation='sigmoid'))\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def define_gan(self):\n",
    "\n",
    "        # make weights in the discriminator not trainable\n",
    "        self.d_model.trainable = False\n",
    "        # connect them\n",
    "        model = Sequential()\n",
    "        # add generator\n",
    "        model.add(self.g_model)\n",
    "        # add the discriminator\n",
    "        model.add(self.d_model)\n",
    "        # compile model\n",
    "        opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "        return model\n",
    "\n",
    "\n",
    "    # generate points in latent space as input for the generator\n",
    "    def generate_latent_points(self, n_samples):\n",
    "        # generate points in the latent space\n",
    "        x_input = rand(self.latent_dim * n_samples)\n",
    "        # reshape into a batch of inputs for the network\n",
    "        x_input = x_input.reshape(n_samples, self.latent_dim)\n",
    "        return x_input\n",
    "\n",
    "\n",
    "    # use the generator to generate n fake examples, with class labels\n",
    "    def generate_fake_samples(self, n_samples):\n",
    "        # generate points in latent space\n",
    "        x_input = self.generate_latent_points(n_samples)\n",
    "        # predict outputs\n",
    "        X = self.g_model.predict(x_input)\n",
    "        # create 'fake' class labels (0)\n",
    "        y = zeros((n_samples, 1))\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    # select real samples\n",
    "    def generate_real_samples(self, dataset, n_samples):\n",
    "        # choose random instances\n",
    "        ix = randint(0, dataset.shape[0], n_samples)\n",
    "        # retrieve selected samples\n",
    "        X = dataset[ix]\n",
    "        # generate 'real' class labels (1)\n",
    "        y = ones((n_samples, 1))\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def summarize_performance(self, epoch, dataset, n_samples=100):\n",
    "        # prepare real samples\n",
    "        X_real, y_real = self.generate_real_samples(dataset, n_samples)\n",
    "        # evaluate discriminator on real examples\n",
    "        _, acc_real = self.d_model.evaluate(X_real, y_real, verbose=0)\n",
    "        # prepare fake examples\n",
    "        x_fake, y_fake = self.generate_fake_samples(n_samples)\n",
    "        # evaluate discriminator on fake examples\n",
    "        _, acc_fake = self.d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "        # summarize discriminator performance\n",
    "        filename = 'generator_model_%03d.h5' % (epoch + 1)\n",
    "        self.g_model.save(filename)\n",
    "        #print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "\n",
    "    def train(self, dataset, n_epochs=100, n_batch=256):\n",
    "        bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "        half_batch = int(n_batch / 2)\n",
    "        # manually enumerate epochs\n",
    "        for i in range(n_epochs):\n",
    "            # enumerate batches over the training set\n",
    "            for j in range(bat_per_epo):\n",
    "                # get randomly selected 'real' samples\n",
    "                X_real, y_real = self.generate_real_samples(dataset, half_batch)\n",
    "                # generate 'fake' examples\n",
    "                X_fake, y_fake = self.generate_fake_samples(half_batch)\n",
    "                # create training set for the discriminator\n",
    "                X, y = np.vstack((X_real, X_fake)), np.vstack((y_real, y_fake))\n",
    "                # update discriminator model weights\n",
    "                d_loss, _ = self.d_model.train_on_batch(X, y)\n",
    "                # prepare points in latent space as input for the generator\n",
    "                X_gan = self.generate_latent_points(n_batch)\n",
    "                # create inverted labels for the fake samples\n",
    "                y_gan = ones((n_batch, 1))\n",
    "                # update the generator via the discriminator's error\n",
    "                g_loss = self.gan_model.train_on_batch(X_gan, y_gan)\n",
    "                # summarize loss on this batch\n",
    "                #print('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))\n",
    "\n",
    "            if (i+1) % 50 == 0:\n",
    "                self.summarize_performance(i, dataset)\n",
    "\n",
    "    def bring_generator(self):\n",
    "\n",
    "        return self.g_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "{'SMOTE': 0.9933333333333333, 'SVMSMOTE': 0.94, 'GAN with LS of 30': 0.12666666666666668, 'GAN with LS of 50': 0.6133333333333333}\n"
     ]
    }
   ],
   "source": [
    "# Experimental procedure\n",
    "\n",
    "# In this section the different configurations are implemented\n",
    "\n",
    "##-------------------------------------------------------------------------------------------\n",
    "\n",
    "features = preprocessing(features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "# Classical models to explore\n",
    "classical_models = [SMOTE, SVMSMOTE]\n",
    "classical_models_str = ['SMOTE',  'SVMSMOTE']\n",
    "\n",
    "# Save the results in a dictionary\n",
    "results = {}\n",
    "\n",
    "# New proposed model\n",
    "new_models_LS = [30, 50]\n",
    "new_models_str = ['GAN with LS of 30', 'GAN with LS of 50']\n",
    "\n",
    "# Iterate through the classical models\n",
    "for i in range(len(classical_models)):\n",
    "\n",
    "\n",
    "    new_features_yes, new_labels_yes = generate_data_classical_models(X_train, y_train, 1, 0, 0.8, classical_models[i])\n",
    "    new_features_no, new_labels_no = generate_data_classical_models(X_train, y_train, 0, 1, 0.5, classical_models[i])\n",
    "\n",
    "    synthetic_data = np.vstack((new_features_yes,new_features_no))\n",
    "    synthetic_labels = np.hstack((new_labels_yes, new_labels_no))\n",
    "    s_X, s_y = shuffle(synthetic_data, synthetic_labels)\n",
    "\n",
    "    # train knn with synthetic data\n",
    "    knn = KNeighborsClassifier(5)\n",
    "\n",
    "    knn.fit(s_X, s_y)\n",
    "\n",
    "    results[classical_models_str[i]] = knn.score(X_test, y_test)\n",
    "\n",
    "for i in range(len(new_models_LS)):\n",
    "\n",
    "    gan_yes = gan1(new_models_LS[i])\n",
    "\n",
    "    gan_yes.train(X_train[y_train == 1], n_epochs=100, n_batch=256)\n",
    "\n",
    "    generator_yes = gan_yes.bring_generator()\n",
    "\n",
    "    num_new_samples = int(X_train[y_train == 1].shape[0] * 0.8)\n",
    "\n",
    "    # generate points\n",
    "    latent_points = gan_yes.generate_latent_points(num_new_samples)\n",
    "\n",
    "    # generate samples\n",
    "    s_X_yes = generator_yes.predict(latent_points)\n",
    "\n",
    "    gan_no= gan1(new_models_LS[i])\n",
    "\n",
    "    gan_no.train(X_train[y_train == 0], n_epochs=100, n_batch=256)\n",
    "\n",
    "    generator_no = gan_no.bring_generator()\n",
    "\n",
    "    num_new_samples = int(X_train[y_train == 0].shape[0] * 0.5)\n",
    "\n",
    "    # generate points\n",
    "    latent_points = gan_no.generate_latent_points(num_new_samples)\n",
    "\n",
    "    # generate samples\n",
    "    s_X_no = generator_no.predict(latent_points)\n",
    "\n",
    "    s_X_temp = np.vstack((s_X_yes, s_X_no))\n",
    "\n",
    "\n",
    "    labels_temp = np.zeros(s_X_temp.shape[0])\n",
    "    labels_temp[0:s_X_yes.shape[0]] = 1\n",
    "\n",
    "    s_X, s_y = shuffle(s_X_temp, labels_temp)\n",
    "\n",
    "    # train knn with synthetic data\n",
    "    knn = KNeighborsClassifier(5)\n",
    "\n",
    "    knn.fit(s_X, s_y)\n",
    "\n",
    "    results[new_models_str[i]] = knn.score(X_test, y_test)\n",
    "\n",
    "\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SMOTE': 0.9933333333333333, 'SVMSMOTE': 0.94, 'GAN with LS of 30': 0.12666666666666668, 'GAN with LS of 50': 0.6133333333333333}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
